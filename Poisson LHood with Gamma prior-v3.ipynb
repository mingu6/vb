{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seed for prng2 of 11 + 12 doesn't really converge, 13 does! RP trick tends to yield less noisy gradients but convergence slower.\n",
    "\n",
    "Convergence depends on initial conditions\n",
    "\n",
    "To do: Natural gradient, write up commentary, benchmarks, cython\n",
    "\n",
    "Large parameters cause gradients to be too large!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import RandomState\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import scipy.special as s_spec\n",
    "import scipy\n",
    "from numpy import sqrt\n",
    "from numpy import exp\n",
    "from numpy import log\n",
    "\n",
    "###############generate simulated dataset from Poisson distribution###############\n",
    "\n",
    "#set parameters \n",
    "lambda_data =  5\n",
    "n = 20 #no. of samplest\n",
    "\n",
    "#generate dataset\n",
    "prng1 = RandomState(0)\n",
    "data = prng1.poisson(lambda_data, n)\n",
    "x_bar = np.mean(data)\n",
    "s = np.var(data) #variance is divided by n not n-1\n",
    "\n",
    "#set prior distribution parameters\n",
    "a_0 = 4\n",
    "b_0 = 0.7\n",
    "\n",
    "#initialise initial variational parameter estimates\n",
    "prng2 = RandomState(13)\n",
    "\n",
    "#set parameters for MC gradient evaluation\n",
    "n_samples = 100\n",
    "learn = 0.001\n",
    "n_iter = 7000 #iterations for gradient ascent\n",
    "#set generalised gradient parameters\n",
    "n_samples_grg = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now characterise the posterior analytically. First, we assume there are $n$ i.i.d observations and they follow a Poisson distribution so\n",
    "\n",
    "$L(\\textbf{x} | \\lambda) = \\prod_{i=1}^{n} \\frac{\\lambda^{x_i}e^\\lambda}{x_i!}$ \n",
    "\n",
    "Next, we assume the $\\lambda$ has a gamma prior distribution \n",
    "\n",
    "$\\lambda \\sim Gamma(\\alpha_0,\\beta_0)$\n",
    "\n",
    "where for $X\\sim Gamma(\\alpha,\\beta)$, we use the Gamma density of the form $f(x|\\alpha,\\beta)=\\frac{\\beta^\\alpha}{\\Gamma{(\\alpha})}x^{\\alpha-1}e^{\\beta x}$. Given this likelihood and prior, the posterior is also a gamma distribution\n",
    "\n",
    "$\\lambda|\\textbf{x}\\sim Gamma(\\alpha_0+\\sum_{i=1}^n x_i,\\beta_0+n)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The posterior parameters are: (alpha = 100.00, beta = 20.70)\n"
     ]
    }
   ],
   "source": [
    "###############Calculate analytically posterior parameters###############\n",
    "\n",
    "#see above for formula\n",
    "a_pos = a_0+x_bar*n\n",
    "b_pos = b_0+n\n",
    "\n",
    "print('The posterior parameters are: (alpha = {:0.2f}, beta = {:0.2f})'.format(a_pos, b_pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume the form of the VB approximation of the posterior is \n",
    "\n",
    "$q(\\textbf{z};\\textbf{v})=\\frac{{\\beta_n}^{\\alpha_n}}{\\Gamma{(\\alpha_n})}\\lambda^{\\alpha_n-1}e^{\\beta_n \\lambda}$\n",
    "\n",
    "We seek to find the optimal value of the parameter vector $\\textbf{v}=[\\alpha_n,\\beta_n]$ by minimising the KL divergence between $q$ and the actual posterior. In this case, the parameters should match exactly since the posterior is also a gamma distribution.\n",
    "\n",
    "To find the optimal values for the parameters, we seek to maximise the variational lower bound $\\mathcal{L}(\\textbf{v})$ w.r.t. $\\textbf{v}$ where \n",
    "\n",
    "$\\mathcal{L}(\\textbf{v}) = [\\log p(\\textbf{x},\\textbf{z}) - \\log q(\\textbf{z};\\textbf{v})] = \\mathbb{E}_{q(\\textbf{z},\\textbf{v})}[f(\\textbf{z})]+\\mathbb{H}_{q(\\textbf{z},\\textbf{v})}[q(\\textbf{z},\\textbf{v})]$\n",
    "\n",
    "where $\\textbf{x}$ is the data, $\\textbf{z}=\\lambda$, and $\\mathbb{H}_{q(\\textbf{z},\\textbf{v})}[q(\\textbf{z},\\textbf{v})]$ is the entropy of $q(\\textbf{z},\\textbf{v})$.\n",
    "\n",
    "and $f(\\textbf{z})=\\log p(\\textbf{x},\\textbf{z})=\\sum_{i=1}^n \\log p(x_i|\\textbf{z}) + \\log p(\\textbf{z};\\textbf{v})$\n",
    "\n",
    "We use some form of gradient ascent to maximise the lower bound, and from the literature there are two different ways to do this. The first way is called the *score function* method which utilises the log-derivative trick to estimate the gradient. This method supposedly has high variance and is in general not able to reliably estimate the gradient. The second way is using the *generalised reparameterisation gradient* outlined in the paper by Ruiz et. al. (2016). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To estimate $\\nabla_{\\textbf{v}}\\mathcal{L}$ using the score function method, we seek to evaluate\n",
    "\n",
    "$\\nabla_{\\textbf{v}}\\mathcal{L}=\\nabla_{\\textbf{v}}(\\mathbb{E}_{q(\\textbf{z},\\textbf{v})}[f(\\textbf{z})]+\\mathbb{H}_{q(\\textbf{z},\\textbf{v})}[q(\\textbf{z},\\textbf{v})])=\\nabla_{\\textbf{v}}\\mathbb{E}_{q(\\textbf{z}\n",
    ",\\textbf{v})}[f(\\textbf{z})]+\\nabla_{\\textbf{v}}\\mathbb{H}_{q(\\textbf{z},\\textbf{v})}[q(\\textbf{z},\\textbf{v})]$\n",
    "\n",
    "We have that $\\nabla_{\\textbf{v}}\\mathbb{E}_{q(\\textbf{z},\\textbf{v})}[f(\\textbf{z})]=\\nabla_{\\textbf{v}}\\int q(\\textbf{z},\\textbf{v})f(\\textbf{z}) dz=\\int \\nabla_{\\textbf{v}} q(\\textbf{z},\\textbf{v})f(\\textbf{z}) dz=\\int \\frac{q(\\textbf{z},\\textbf{v})}{q(\\textbf{z},\\textbf{v})}\\nabla_{\\textbf{v}} q(\\textbf{z},\\textbf{v})f(\\textbf{z}) dz=\\int q(\\textbf{z},\\textbf{v})f(\\textbf{z})\\nabla_{\\textbf{v}}\\log q(\\textbf{z},\\textbf{v}) = \\mathbb{E}_{q(\\textbf{z},\\textbf{v})}[f(\\textbf{z})\\nabla_{\\textbf{v}}\\log q(\\textbf{z},\\textbf{v})]$\n",
    "\n",
    "We now need to estimate the final term above and the gradient of the entropy. First, given our choices of likelihood, prior and $q$, we have that\n",
    "\n",
    "$f(\\textbf{z})=\\log p(\\textbf{x},\\textbf{z}) = \\alpha_0 \\log{\\beta_0}-\\log{\\Gamma(\\alpha_0)}-\\sum_{i=1}^n x_i! + (\\sum_{i=1}^n x_i + \\alpha_0 - 1)\\log{\\textbf{z}}-(\\beta_0+n)\\textbf{z}$ \n",
    "\n",
    "Also, $\\nabla_{\\textbf{v}}\\log q(\\textbf{z},\\textbf{v})=[\\frac{\\partial}{\\partial\\alpha_n}\\log q(\\textbf{z},\\textbf{v}), \\frac{\\partial}{\\partial\\beta_n}\\log q(\\textbf{z},\\textbf{v})]^{\\intercal}$ where\n",
    "\n",
    "$\\frac{\\partial}{\\partial\\alpha_n}\\log q(\\textbf{z},\\textbf{v}) = \\log \\beta_n - \\psi(\\alpha_n)+\\log(\\textbf{z})$ where $\\psi$ is the digamma function.\n",
    "\n",
    "$\\frac{\\partial}{\\partial\\beta_n}\\log q(\\textbf{z},\\textbf{v})=\\frac{\\alpha_n}{\\beta_n}-\\textbf{z}$\n",
    "\n",
    "Finally, the entropy can be shown to be\n",
    "\n",
    "$\\mathbb{H}_{q(\\textbf{z},\\textbf{v})}[q(\\textbf{z},\\textbf{v})]=\\alpha_n-\\log \\beta_n + \\log \\Gamma(\\alpha_n) + (1-\\alpha_n)\\psi(\\alpha_n)$\n",
    "\n",
    "and so we have that the gradient of the entropy is \n",
    "\n",
    "$\\nabla_{\\textbf{v}}\\mathbb{H}_{q(\\textbf{z},\\textbf{v})}[q(\\textbf{z},\\textbf{v})]=[\\frac{\\partial}{\\partial\\alpha_n}\\mathbb{H}_{q(\\textbf{z},\\textbf{v})},\\frac{\\partial}{\\partial\\beta_n}\\mathbb{H}_{q(\\textbf{z},\\textbf{v})}]^\\intercal$ where\n",
    "\n",
    "$\\frac{\\partial}{\\partial\\alpha_n}\\mathbb{H}_{q(\\textbf{z},\\textbf{v})}=1-\\alpha_n \\psi_1(\\alpha_n)$ where $\\psi_n$ is the $n^{th}$ derivative of the digamma function and\n",
    "\n",
    "$\\frac{\\partial}{\\partial\\beta_n}\\mathbb{H}_{q(\\textbf{z},\\textbf{v})} = -\\frac{1}{\\beta_n}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(z):\n",
    "    return a_0*np.log(b_0)-np.log(s_spec.digamma(a_0))-np.sum(np.log(scipy.misc.factorial(data)))+ \\\n",
    "(n*x_bar+a_0-1)*np.log(z)-(b_0+n)*z\n",
    "\n",
    "#transforms variational parameters onto the real line\n",
    "def T_v(xi):\n",
    "    v = exp(xi)\n",
    "    return v\n",
    "\n",
    "def T_inv_v(v):\n",
    "    xi = log(v)\n",
    "    return v\n",
    "\n",
    "def T_v_grad(xi):\n",
    "    v_g = exp(xi)\n",
    "    return v_g\n",
    "\n",
    "#calculate elements of the ELBO gradient\n",
    "\n",
    "def entropy_grad(xi):\n",
    "    grad = np.empty(2)\n",
    "    #retrieve parameters\n",
    "    v = T_v(xi)\n",
    "    a = v[0]\n",
    "    b = v[1]\n",
    "    #retrieve gradient of transform\n",
    "    v_g = T_v_grad(xi)\n",
    "    print('v_g',v_g)\n",
    "    #calculate partial derivatives of entropy\n",
    "    grad[0] = (1 + (1-a)*s_spec.polygamma(1,a))*v_g[0]\n",
    "    grad[1] = -1/b*v_g[1]\n",
    "    return grad\n",
    "\n",
    "def ln_q_deriv_xi(z,xi):\n",
    "    #retrieve parameters\n",
    "    v = T_v(xi)\n",
    "    a = v[0]\n",
    "    b = v[1]\n",
    "    #retrieve gradient of transform\n",
    "    v_g = T_v_grad(xi)\n",
    "    #evaluate final gradient\n",
    "    d_ln_q = np.empty([2,z.size])\n",
    "    d_ln_q[0] = (np.log(b)-s_spec.digamma(a)+np.log(z))*v_g[0]\n",
    "    d_ln_q[1] = (a/b-z)*v_g[1]\n",
    "    return d_ln_q\n",
    "\n",
    "def lb_grad_score(z, xi):\n",
    "    grad = np.empty([2,z.size])\n",
    "    #retrieve parameters\n",
    "    v = T_v(xi)\n",
    "    a_n = v[0]\n",
    "    b_n = v[1]\n",
    "    #calculate partial derivatives of log(q(z;v))\n",
    "    q_del = ln_q_deriv_xi(z,v) \n",
    "    #set gradient\n",
    "    grad[0] = f(z)*q_del[0]\n",
    "    grad[1] = f(z)*q_del[1]\n",
    "    return grad\n",
    "\n",
    "def ELBO(xi):\n",
    "    #retrieve parameters\n",
    "    v = T_v(xi)\n",
    "    a_n = v[0]\n",
    "    b_n = v[1]\n",
    "    #evaluate expectation of f(z) under q (feed expectation through z terms)\n",
    "    E_f = a_0*np.log(b_0)-np.log(s_spec.digamma(a_0))-np.sum(np.log(scipy.misc.factorial(data)))+ \\\n",
    "    (n*x_bar+a_0-1)*(s_spec.digamma(a_n) - np.log(b_n))-(b_0+n)*a_n/b_n\n",
    "    entropy = a_n-np.log(b_n)+np.log(s_spec.gamma(a_n))+(1-a_n)*s_spec.digamma(a_n)\n",
    "    cost = E_f + entropy\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised parameters:  [ 1.2921144   1.41563928]\n",
      "v [ 1.2921144   1.41563928]\n",
      "v_g [ 1.2921144   1.41563928]\n",
      "grad [ 461.62641218 -177.0030442 ]\n",
      "v [ 130.65288467    0.24112273]\n",
      "v_g [ 130.65288467    0.24112273]\n",
      "grad [  7.29490243e+62  -5.87745717e+60]\n",
      "v [ inf   0.]\n",
      "v_g [ inf   0.]\n",
      "grad [ nan  nan]\n",
      "v [ nan  nan]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mingxu/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:6: RuntimeWarning: overflow encountered in exp\n",
      "/Users/mingxu/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:65: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/mingxu/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:65: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/Users/mingxu/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:66: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/mingxu/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:66: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/Users/mingxu/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:17: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "/Users/mingxu/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:42: RuntimeWarning: invalid value encountered in add\n",
      "/Users/mingxu/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:43: RuntimeWarning: invalid value encountered in subtract\n",
      "/Users/mingxu/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:2: RuntimeWarning: invalid value encountered in subtract\n",
      "  from ipykernel import kernelapp as app\n",
      "/Users/mingxu/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:14: RuntimeWarning: overflow encountered in exp\n",
      "/Users/mingxu/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:29: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/Users/mingxu/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:30: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "/Users/mingxu/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:30: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shape < 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-59e2c94e0d1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m#print(i)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'v'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mgrad_vect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlb_grad_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#for variance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_vect\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mentropy_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.gamma (numpy/random/mtrand/mtrand.c:22789)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shape < 0"
     ]
    }
   ],
   "source": [
    "###############estimate gradient using MC###############\n",
    "\n",
    "#initialise parameters randomly\n",
    "xi_0 = prng2.uniform(0,1,2)\n",
    "xi = xi_0\n",
    "#convert using transformation to parameter space\n",
    "v = T_v(xi)\n",
    "#print initialisation of parameter\n",
    "print(\"Initialised parameters: \",v)\n",
    "#initialise graph vectors\n",
    "cost_graph = np.empty(n_iter)\n",
    "grad_var = np.empty([n_iter,2])\n",
    "\n",
    "for i in range(0,n_iter):\n",
    "    #print(i)\n",
    "    print('v',v)\n",
    "    samples = np.random.gamma(v[0],1/v[1],n_samples)\n",
    "    grad_vect = lb_grad_score(samples, xi) #for variance\n",
    "    grad = np.mean(grad_vect,axis=1) + entropy_grad(xi)\n",
    "    #update variance of gradient for comparison to RP\n",
    "    grad_var[i] = np.var(grad_vect, axis=1)\n",
    "    #update transformed variational parameters\n",
    "    print('grad',grad)\n",
    "    xi = xi + learn*grad\n",
    "    #convert back to parameter space\n",
    "    v = T_v(xi)\n",
    "    #record the cost for plotting to check convergence\n",
    "    c = ELBO(xi)\n",
    "    cost_graph[i]= c\n",
    "    \n",
    "print(\"Final variational parameters: \",v)\n",
    "\n",
    "#check convergence\n",
    "\n",
    "plt.plot(np.linspace(1,n_iter,n_iter), cost_graph, label='Score function method')\n",
    "plt.ylabel('ELBO')\n",
    "plt.xlabel('n')\n",
    "plt.title('Convergence of ELBO')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#summary metrics - compare to analytical result\n",
    "print('Converged ELBO: ',c) \n",
    "print('ELBO of analytical posterior parameters: ', ELBO(T_inv_v([a_pos,b_pos])))\n",
    "print('Mean of VB approximation',v[0]/v[1])\n",
    "print('Mean of analytic posterior: ',a_pos/b_pos)\n",
    "\n",
    "#set up plot for pdf of posterior under VB\n",
    "vb_mean = v[0]/v[1]\n",
    "vb_sd = vb_mean/v[1]\n",
    "x = np.linspace(vb_mean-8*vb_sd, vb_mean+8*vb_sd, 200) #generate sample space for plotting density\n",
    "pdf_vb = stats.gamma.pdf(x, v[0], scale=1/v[1])\n",
    "\n",
    "#set up plot for pdf of analytical posterior\n",
    "a_mean = a_pos/b_pos\n",
    "a_sd = a_mean/b_pos\n",
    "#x = np.linspace(a_mean-4*a_sd, a_mean+4*a_sd, 200)\n",
    "pdf_a = stats.gamma.pdf(x, a_pos, scale=1/b_pos)\n",
    "\n",
    "#conditional/marginal for tau\n",
    "plt.plot(x, pdf_a, 'b', label='Analytical')\n",
    "plt.plot(x, pdf_vb, 'r', label='VB approximation')\n",
    "plt.ylabel('p(x)')\n",
    "plt.xlabel('x')\n",
    "plt.legend()\n",
    "plt.title(r'$\\tau$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the generalised reparameterisation gradient by Ruiz et. al. to try and find a low variance estimate of the gradient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def T(e, xi):\n",
    "    #retrieve parameters\n",
    "    v = T_v(xi)\n",
    "    a_n = v[0]\n",
    "    b_n = v[1]\n",
    "    return np.exp(e*np.sqrt(s_spec.polygamma(1,a_n))+s_spec.digamma(a_n)-np.log(b_n))\n",
    "\n",
    "def Tinv(z, xi):\n",
    "    #retrieve parameters\n",
    "    v = T_v(xi)\n",
    "    a_n = v[0]\n",
    "    b_n = v[1]\n",
    "    return (np.log(z)-s_spec.digamma(a_n) + np.log(b_n))/np.sqrt(s_spec.polygamma(1,a_n))\n",
    "\n",
    "def h(e, xi):\n",
    "    #retrieve parameters\n",
    "    v = T_v(xi)\n",
    "    a = v[0]\n",
    "    b = v[1]\n",
    "    h = np.empty([2,e.size])\n",
    "    pg_1 = s_spec.polygamma(1,a)\n",
    "    #bring in gradient of T_v\n",
    "    v_g = T_v_grad(xi)\n",
    "    #calculate h\n",
    "    h[0] = T(e, v)*(e*s_spec.polygamma(2,a)/(2*np.sqrt(pg_1))+ pg_1)*v_g[0]\n",
    "    h[1] = -T(e, v)/b*v_g[1]\n",
    "    return h\n",
    "\n",
    "def u(e, v):\n",
    "    #retrieve parameters\n",
    "    v = T_v(xi)\n",
    "    a = v[0]\n",
    "    b = v[1]\n",
    "    #bring in gradient of T_v\n",
    "    v_g = T_v_grad(xi)\n",
    "    #set up output for u\n",
    "    u = np.empty([2,e.size])\n",
    "    pg_1 = s_spec.polygamma(1,a)\n",
    "    pg_2 = s_spec.polygamma(2,a)\n",
    "    u[0] = (e*pg_2/(2*np.sqrt(pg_1))+ pg_1 + pg_2/(2*pg_1))*v_g[0]\n",
    "    u[1] = -1/b*v_g[1]\n",
    "    return u\n",
    "\n",
    "#calculate Fisher information of variational distribution for natural gradient\n",
    "def I(xi):\n",
    "    #find information w.r.t. v\n",
    "    v=T_v(xi)\n",
    "    a = v[0]\n",
    "    b = v[1]\n",
    "    I_v = np.zeros([2,2])\n",
    "    I_v[0,0] = -s_spec.polygamma(1,a); I_v[1,1]=-a/b**2\n",
    "    I_v[0,1]=1/b; I_v[1,0]=1/b\n",
    "    #evaluate Jacobian of transformation\n",
    "    J = T_v_J(xi)\n",
    "    return np.matmul(np.matmul(np.transpose(J),I_v),J)\n",
    "\n",
    "#evaluate Jacobian of transformation of variation parameters for natural gradient\n",
    "\n",
    "def T_v_J(xi):\n",
    "    v_g = exp(xi)\n",
    "    #initalise Jacobian\n",
    "    J = np.zeros([2,2])\n",
    "    J[0,0]=v_g[0]\n",
    "    J[1,1]=v_g[1]\n",
    "    return J\n",
    "\n",
    "def f_deriv(z):\n",
    "    return (x_bar*n+a_0-1)/z-(b_0+n)\n",
    "\n",
    "def ln_q_deriv_z(z,v):\n",
    "    a = v[0]\n",
    "    b = v[1]\n",
    "    return (a-1)/z-b\n",
    "\n",
    "def g_corr(z, v):\n",
    "    g_corr = np.empty([2,z.size])\n",
    "    g_corr = f(z)*(ln_q_deriv_z(z,v)*h(Tinv(z,v),v)+ln_q_deriv_xi(z,v)+u(Tinv(z,v),v))\n",
    "    return g_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#initalise parameters consistent with score method\n",
    "xi = xi_0\n",
    "v = T_v(xi)\n",
    "print(\"Initialised parameters: \",v)\n",
    "#initialise graphing arrays\n",
    "cost_graph1 = np.empty(n_iter)\n",
    "grad_var1 = np.empty([n_iter,2])\n",
    "grad_mean1 = np.empty([n_iter,2])\n",
    "grad_cv1 = np.empty([n_iter,2])\n",
    "\n",
    "for i in range(0,n_iter):\n",
    "    samples = np.random.gamma(v[0],1/v[1],n_samples_grg)\n",
    "    g_rep = f_deriv(samples)*h(Tinv(samples,xi),xi)\n",
    "    g_cor = g_corr(samples,xi)\n",
    "    #calculate gradient\n",
    "    grad_vect = g_rep + g_cor\n",
    "    grad_rp = np.mean(grad_vect) + entropy_grad(xi)\n",
    "    #store variance of gradient to compare to score\n",
    "    grad_var1[i] = np.var(grad_vect, axis=1)\n",
    "    #update transformed variational parameters\n",
    "    xi = xi + learn*grad_rp\n",
    "    v = T_v(xi)\n",
    "    #record the cost for plotting to check convergence\n",
    "    c = ELBO(xi)\n",
    "    cost_graph1[i]= c\n",
    "\n",
    "print(\"Final variational parameters: \",v)\n",
    "\n",
    "#check convergence of ELBO\n",
    "plt.plot(np.linspace(1,n_iter,n_iter), cost_graph, label='Score function method')\n",
    "plt.plot(np.linspace(1,n_iter,n_iter), cost_graph1, label='Reparameterisation trick')\n",
    "plt.ylabel('ELBO')\n",
    "plt.xlabel('n')\n",
    "plt.legend()\n",
    "plt.title('Convergence of ELBO')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#analysis of gradient variance graphs\n",
    "plt.title(\"Variance of gradient by iteration - reparameterisation\")\n",
    "plt.hist(grad_var1,bins=50)\n",
    "plt.show()\n",
    "\n",
    "plt.title(\"Variance of gradient by iteration - score\")\n",
    "plt.hist(grad_var,bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#summary metrics - compare to analytical result\n",
    "print('Converged ELBO: ',c) \n",
    "print('ELBO of analytical posterior parameters: ', ELBO(T_inv_v([a_pos,b_pos])))\n",
    "print('Mean of VB approximation',v[0]/v[1])\n",
    "print('Mean of analytic posterior: ',a_pos/b_pos)\n",
    "\n",
    "#set up plot for pdf of posterior under VB\n",
    "vb_rp_mean = v[0]/v[1]\n",
    "vb_rp_sd = vb_mean/v[1]\n",
    "#x = np.linspace(0,150,200)\n",
    "x = np.linspace(vb_mean-10*vb_sd, vb_mean+10*vb_sd, 200) #generate sample space for plotting density\n",
    "pdf_vb_rp = stats.gamma.pdf(x, v[0], scale=1/v[1])\n",
    "\n",
    "#set up plot for pdf of analytical posterior\n",
    "#x = np.linspace(a_mean-4*a_sd, a_mean+4*a_sd, 200)\n",
    "pdf_a = stats.gamma.pdf(x, a_pos, scale=1/b_pos)\n",
    "\n",
    "#conditional/marginal for tau\n",
    "plt.plot(x, pdf_a, 'b', label='Analytical')\n",
    "plt.plot(x, pdf_vb_rp, 'r', label='VB approximation')\n",
    "plt.ylabel('p(x)')\n",
    "plt.xlabel('x')\n",
    "plt.legend()\n",
    "plt.title(r'$\\tau$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now implement the reparameterisation trick using the natural gradient instead of the standard Euclidean gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#initalise parameters consistent with score method\n",
    "xi = xi_0\n",
    "v = T_v(xi)\n",
    "print(\"Initialised parameters: \",v)\n",
    "#initialise graphing arrays\n",
    "cost_graph2 = np.empty(n_iter)\n",
    "grad_var1 = np.empty([n_iter,2])\n",
    "\n",
    "for i in range(0,5):\n",
    "    samples = np.random.gamma(v[0],1/v[1],n_samples_grg)\n",
    "    g_rep = f_deriv(samples)*h(Tinv(samples,xi),xi)\n",
    "    g_cor = g_corr(samples,xi)\n",
    "    #calculate gradient\n",
    "    grad_vect = g_rep + g_cor\n",
    "    grad_rp = np.mean(grad_vect) + entropy_grad(xi)\n",
    "    #store variance of gradient to compare to score\n",
    "    grad_var1[i] = np.var(grad_vect, axis=1)\n",
    "    #calculate Fisher information matrix for natural gradient\n",
    "    xi_I = I(xi)\n",
    "    print(xi_I,np.linalg.inv(xi_I),T_v_J(xi), xi)\n",
    "    #update transformed variational parameters\n",
    "    xi = xi + learn*np.matmul(np.linalg.inv(xi_I),grad_rp)\n",
    "    v = T_v(xi)\n",
    "    #record the cost for plotting to check convergence\n",
    "    c = ELBO(xi)\n",
    "    cost_graph2[i]= c\n",
    "\n",
    "print(\"Final variational parameters: \",v)\n",
    "\n",
    "#check convergence of ELBO\n",
    "plt.plot(np.linspace(1,n_iter,n_iter), cost_graph, label='Score method')\n",
    "plt.plot(np.linspace(1,n_iter,n_iter), cost_graph1, label='RP trick')\n",
    "plt.plot(np.linspace(1,n_iter,n_iter), cost_graph2, label='RP trick w/Natural gradient')\n",
    "plt.ylabel('ELBO')\n",
    "plt.xlabel('n')\n",
    "plt.legend()\n",
    "plt.title('Convergence of ELBO')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
